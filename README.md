# Telegram
## MVP
- Отправка личного сообщения
- Прикрепление фотографии в сообщении
- Груповые чаты
## Целевая аудитория:
- 500 млн человек активных пользователей в месяц
- 55 млн человек активных пользователей в день
- 15 миллиардов сообщений каждый день
- 175 миллионов фотографий ежедневно
- Северная Америка, Россия, Европа, Азия
## Расчет нагрузки:
1. #### Расчет нагрузки на отправку сообщений
Каждый день пользователь отправляет 300 млн человек оправляют 30 миллиардов сообщений.
Считать нагрузку будем среднюю за сутки и в часы наибольшей активности аудитории, это приблизительно с 9 до 23 итого 14 часов

Среднее за сутки:

    15 000 000 000 / (24 * 3600) = 173 612 запросов в секунду
    
В период наибольшей активности:

    15 000 000 000 / (14 * 3600) = 297 620 запросов в секунду

Средняя длинна сообщения в мессенджере 70 символов, для небольшого запаса возьмем 100.
Один символ в unicode занимайт 2 байта + 1 байт служебной информации, посчитаем траффик на получение сообщений

Среднее за сутки:

    3 * 100 * 15 000 000 000 / (24 * 3600) = 0.39 Гбит/с
    
В период наибольшей активности:

    3 * 100 * 15 000 000 000 / (14 * 3600) = 0.67 Гбит/с
    
Для расчета траффика на отправку необходимо учесть, что часть сообщений будет отправленно в групповые чате,
в результате чего доставлять их нужно будет всем пользователям в данном группе.
Предположим, что в среднем 1/3 от всех сообщений отправляется в групповые чаты. На начальных этапах до сбора полноценной статистики
ограничим количество людей в одной группе, сделаем равным 100 человек, считать будем по максимальному значению людей в одной группе

Среднее за сутки:

    3 * 100 * 10 000 000 000 / (24 * 3600) + 3 * 100 * 5 000 000 000 * 99 / (24 * 3600) = 13.06 Гбит/с
    
В период наибольшей активности:

    3 * 100 * 10 000 000 000 / (14 * 3600) + 3 * 100 * 5 000 000 000 * 99 / (14 * 3600) = 22.4 Гбит/с
    
Зная, что одно сообщение в среднем весит 300 байт, то нам нужно следующего размера хранилище на пользователей в месяц:

    300 * 15 000 000 000 * 30 = 122.8 Тбайт

2. #### Рассчет нагрузки на отправку фото
Ежедневно отправляется около 600 миллионов фото, считаем RPS в период наибольше активности

Среднее за сутки:

    175 000 000 / (24 * 3600) = 2 026 запросов в секунду
    
В период наибольшей активности:

    175 000 000 / (14 * 3600) = 3 472 запросов в секунду
    
Отправленые фото будем сжимать, считая, что в среднем вес одной картинки 500 Кбайт получаем, считаем траффик на получение
Среднее за сутки:

    500 * 175 000 000 / (14 * 3600) = 7.73 Гбит/с
    
В период наибольшей активности:
    
    500 * 175 000 000 / (14 * 3600) = 13.25 Гбит/с
    
Для рассчета траффика отправку, воспользуемся аналогичным методом из пункта про сообщения

Среднее за сутки:

    500 * 116,666,667 / (24 * 3600) + 500 * 58,333,333 * 99 / (24 * 3600) = 260.13 Гбит/с
    
В период наибольшей активности:

    500 * 116,666,667 / (14 * 3600) + 500 * 58,333,333 * 99 / (14 * 3600) = 445.93 Гбит/с
    
Тогда аналогично расчету выше посчитаем размер хранилища на всех пользователей в месяц:

    500 * 175 000 000 * 30 = 2.39 Тбайт

3. #### Рассчет авторизация пользователей:
 
Будем считать, что в среднем пользователь авторизуется 0.25 раз за день, тк у большинства пользователей

Считаем RPS:

В среднем за сутки:

    55 000 000 * 0.25 / (24 * 3600) = 160 запросов в секунду
    
В период наибольшей активности:

    55 000 000 * 0.25 / (14 * 3600) = 273 запроса в секунду
    
Пользователи будут авторизироваться по логину + паролю, которые отправляются в http запросе
Такой запрос состоящий из двух полей будет весить не больше 1 Кбайта
На каждый входящий запрос будет один исходящий, поэтому и на принятие и на отправку нагрузка на сеть будет одинаковой:

В среднем за сутки:

    1 * 55 000 000 * 0.25 / (24 * 3600) = 0.00122 Гбит/с

В период наибольше активности:

    1 * 55 000 000 * 0.25 / (14 * 3600) = 0.0021 Гбит/с

Для хранения будем использовать NoSQL базу данных, в которой будет хранится ключ + значение (логин + пароль)
В таком базе поле будет весить тоже не более 1 КБайта. В этом случае для хранения базы всех пользователей нам потребуется:

    500 000 000 * 1 = 0.466 Тбайт
    
4. #### Расчет получения профиля пользователя
Предположим, что каждый ползователь в день один раз загружает свой профиль и 2 раза открывает чей-то чужой. Те в сумме 3 запроса
Более точные данные будем брать согласно нашей статистике после какого-то времени эксплуатации

Считаем RPS:

В среднем за сутки:
    
    55 000 000 * 3 / (24 * 3600) = 1 909 запросов в секунду

В период наибольше активности:

    55 000 000 * 3 / (14 * 3600) = 3 274 запроса в секунду
    
Размер профиля возьмем равным одной фотографии + логин + описание
Размер одной картинки в среднем - 500 КБайт, всех полей не более 2 КБайт и того 502 Кбайта на 1 профиль
Тут количество траффика на получение и отправку будет так же одинаковое

В среднем за сутки:
 
    55 000 000 * 3 * 502 / (24 * 3600) = 7.3 Гбит/с
    
В период наибольше активности:
    
    55 000 000 * 502 / (24 * 3600) = 12.6 Гбит/с
    
Для хранения всех профилей нам потребуется:

    55 000 000 * 502 = 25.72 Тбайт

Итоговая таблица:
|| RPS в среднем| RPS пиковое значение | Пиковый трафик получения в среднем, Гбит/с | Пиковый трафик получения пиковый, Гбит/с | Пиковый трафик отдачи в среднем, Гбит/с | Пиковый трафик отдачи пиковый, Гбит/с | Размер хранилища в месяц, Тбайт |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Сообщения | 173 612 | 297 620  | 0.39 | 0.67 |13.06  |22.4 |122.8|
| Картинки | 2 025 | 3 472 | 7.73 | 13.25| 260.13|445.93|2.39|
| Авторизация | 160|273 | 0.00122| 0.0021|0.00122 |0.0021|0.466|
| Получение профиля | 1 909| 3 274| 7.3| 12.6|7.3 |12.6|25.72|

## Логическая схема:
<img width="521" alt="Снимок экрана 2021-12-11 в 23 10 07" src="https://user-images.githubusercontent.com/64102363/145690223-dbf8eb4d-bd49-48c4-9714-6f3331a0d1fc.png">

## Физическая схема:
<img width="750" alt="Снимок экрана 2021-12-18 в 14 52 41" src="https://user-images.githubusercontent.com/64102363/146640807-427e406d-2ec9-4b4b-84f1-5fc9f061c69b.png">

Для хранения всех сущностей будем использовать in-memory tarantool в качестве основной базы данных, тк в пики нагрузки у нас ожидается 300к rps и обычная реляционная база врид ли справится с такой нагрузкой, tarantool это noSql решение которое выдерживает очень большие нагрузки и из коробки поддерживает шардинг [15]

Тарантул для поиска кортежей по спейсу требует создавать индексы, создадим индексы по всем id полям всех таблиц(спейсов) а так же по зависимостям из других таблиц (аналог foreign key из sql). В целом данных индексов нам должно хватит для основной части запросов, но если какие-то запросы будут проседять, то можно добавить индекс уже на ходу

Один сервер, вряд ли сможет выдержать нашу нагрузку, поэтому необходимо делать шардинг, 
Шардинг таблицы сообщений будем производить по полю chat_id, тк сообщения там в любой случается потребуются из одного чата и так будет проще делать select
Для чатов и пользователей можно сделать шардинг по id

Помимо шардинга имеет смысл сделать master-slave репликацию, во-первых для большей отказоустойчивости, во-вторых, те данные, что не записываются и не требуют моментального чтения можно будет читать во slave сервера, сейчас у нас нет таких данных, но при разрастании сервиса и добавления новых фич, они могут появится
Так же для уменьшения походов по нескольким базам, произведем денормализацию: в таблицу сообщений добавим поля имя отправителя, и линк фотографии (при необходимости), потому что эти поля всегда будут идти вместе, а значит стоит их хранить вместе.

Для хранения фотографий будем использовать amazom S3 хранилище, а в базе будем хранить лишь ссылку, которую и будем отдавать на клиент

## Технологии

| Технология| Область применения | Мотивационная часть |
| :---: | :---: | :--- |
| TypeScript | Frontend | Фактически стандарт для веба сейчас, преимущество перед JS в наличие статической типизации|
| GO | Backend | Асинхронность из коробки в виде горутин, которыми легко управлять, быстрая скорость компиляции, простота разработки, в случае какого-то узкого можно - возможность использования cgo для написании куска кода на другом языке (например C)  |
| C++ QT| Клиент для windows и linux | Хорошее кроссплатформенное решения для написания клиентских приложений, которые необходимо иметь на всех платформах|
| Swift | IOS | Тут альтернатив немного, использовать ненативную разработку на мой взгляд - боль для конечного потребителя | 
| Kotlin | Android | Аналагичная ситутаяция как и со swift |
| Swift | MacOS | Очень удобно будет для конечного пользователя адаптировать клиент под macOS путем небольшой правки исходного кода IOS приложения, приложение будет гораздо приятнее смотреться и лучше работать, нежели клиент на QT (такую ситуацию сейчас можно наблюдать с telegram)|
| Nginx | Веб-сервер, L7 балансировка, отдача статики | Быстрый асинхронный веб-сервер, легко настраивается |
| Tarantool | База данных | Мотивация по выбору описана в предыдущем пункте |
| Amazon S3 | Хранение файлов | Удобное апи для загрузки/отдачи файлов, скорость работы, масштабируемость |
| Centos | Серверная ОС | Имеет ряд преимуществ перед стандартной ubuntu (например rpm пакеты, которые удобно выкатывать в прод) подробное сравнение в [7]|
| Consul | Для онлайн конфигурации | Удобное распространения конфигов между бекендами, а так же для определения сервисов в L7 балансировке|

## Схема проекта
<img width="560" alt="Снимок экрана 2021-12-18 в 15 44 09" src="https://user-images.githubusercontent.com/64102363/146641538-23f3b6c4-1eea-4952-9c5f-6011375927d6.png">

На картинке видно, как происходит взамодействия внутри системы, а так же разбиение на сервисы, очевидно, что все сервисы так же общаются между собой, но этого не показано, дабы дополнительно не загромождать схему, общение приосходит по протоколу gRPC

В качестве балансировщика на первом этапе выстпупает DNS балансировка, далее балансируем с помощью L7-балансировке на nginx, айпишники серверов получаем из consul
При выборе L7 балансировки опирался на [8]

Отдельно стоит поговорить про строение базы данных, поскольку у нас используеться шардинг, то нужно знать где храниться какая сущность, для этого используется роутер, который знает где лежит какое поле по индексу. Внутри одного шарда данные деляться на виртуальные хранилища - бакеты, и именно по ним роутер и ищет необходимы нам ключ, схема разбивки базы данных на шарды представлена ниже. Так же стоит отметить балансировщик, который при добавлении новых шардов будет перераспределять данные между бакетами. Информация по архитектуре бралась 

<img width="793" alt="Снимок экрана 2021-12-18 в 15 52 18" src="https://user-images.githubusercontent.com/64102363/146641858-66ee1a33-d2c7-4522-a299-8bce2d1c0e26.png">

## Список серверов
Расположения ДЦ:
Выбираем исходя из распределения аудитории, точной статистики по миру на данный момент нет, но например в России [17] находиться 7.5% ползователей

Согласно исследованию [18], большинство аудитории телеграм приходится на азиатские страны, выберем число 60% пользователей

По Северной Америке и Европы нет точных данных по аудитории, возьмум 12.5% по Европе и 15% по северной америке, будет брать по ДЦ на на каждые 7.5% пользователей + один резервный ДЦ на случай если один упадет

Итого:
- Северная Америка: 2 ДЦ + 2 резервных
- Европа: 2 ДЦ + 2 резервных
- Россия: 1 ДЦ + 1 резервный
- Азия: 8 ДЦ + 8 резервных

Постепенно резервные ДЦ можно переводить под основные

### Nginx
Количество rps, приходящиеся на один сервер ~ 310 000

Исходя из rps и веса одного сообщения(300 байт), тк это основная нагрузка, а так же с погрешностью на то, что иногда будут приходить фотографии(500 кб), согласно бенчмаркам из [10] нам достаточно будет 2х ядерных процессоров.

Итоговая конфигурация:
| Параметр| Значение |
| :---: | :---: |
|CPU | 2 ядра |
| Network | 40 Гбит/с |
| RAM | 8 gb |
| Storage | 512 gb | 
 
Узким горлышком является является спобосность пропускного канала, тк исходя из значений, посчитаных ранее, в пиковые часы на отдачу приходится 500Гбит/с

Исходя из этого и будем выбирать количество серверов, те:

    500 / 40 = 13 серверов
    
    
### Сервис авторизации

Тут основным параметром, который необходимо учитывать будет RPS, тк почти все запросы, кроме запросов на картинки, требуют авторизации, поэтому будем считать 
что требуется ~ 310 000rps

Поскольку количество узкое основной логикой будет поход в базу, то нагрузка на cpu будет небольшая, и можно ограничиться 2х ядерными процессорами

Итоговая конфигурация:
| Параметр| Значение |
| :---: | :---: |
| CPU | 2 ядра |
| Network | 40 Гбит/с |
| RAM | 128 gb |
| Storage | 1024 gb |

Исходя из личного опыта, а так же из [11] сервис написанныый на Go, ходящий в базу на стороннем сервере, выдерживает ~ 5 000rps
Итого:

    310 / 5 = 62 серверов
    
### Сервис сообщений  

Тут аналогично сервису авторизации, основная нагрузка идет по запросам в секунду, на сервис сообщений это примерно 300 000 запросов

Но скорее всего этот сервис будет работать помендленнее, тк ходит не в тарантул а в postgres, будет считать что учитывая хорошую индексацию сервис выдержит 4000 rps, если что, после более точного подсчета rps можно легко добавить серверов

Итоговая конфигурация:
| Параметр| Значение |
| :---: | :---: |
| CPU | 2 ядра |
| Network | 40 Гбит/с |
| RAM | 128 gb |
| Storage | 1024 gb |

Итого:

    310 / 4 = 78 серверов

### Сервис картинок

Сервис картинок занимается только загрузкой их на s3 сервер, а значит входящая нагрузка по network будет не такой уж и большой, количество rps тоже сравнимо с одним сервером, поэтому достаточно 2х серверов в запасом, дальше они легко горизонтально масштабируются за счет балансировки

Итоговая конфигурация:
| Параметр| Значение |
| :---: | :---: |
| CPU | 2 ядра |
| Network | 40 Гбит/с |
| RAM | 128 gb |
| Storage | 1024 gb |

Итого:

    2 сервера
    

### Сервис пользователей

Тут аналогично с сервисом картинок, нагрузка не очень велика и сравнима с одим сервером, возьмем опять с запасов 2 штуки, дальше будет горизонтально масшатировать по мере необходимости

Итого:

    2 сервера

### Amazon S3

По rps, тут нагрузка небольшая и исходя из [12] нагрузку выдержит и 1 бакет, но в одном бакете храниться 5ТБ данных [13]
, поэтому их количество нужно считать исходя из него

Ранее мы расчитали, что для хранения картинок, каждый месяц нам необходимо 2.39 ТБ данных в месяц
Поэтому изначально можно взять 5 бакетов и далее их просто закупать по мере необходимости

Итого:

    5 бакетв
    
### Tarantool

Тут есть 2 узких горлышка:

1. Нагрузка по rps
2. Оперативная память

Tarantool - in-memory хранилище, а значит нам нужно все данные хранить в оперативной памяти, поскольку в tarantool мы храним  только key - value в формате user_id - session_hash, то размер поля для одного пользователя будет 8 байт под один uint64(key) и 32 байта под строку (размер строки sha256 строки)

Под хранение нужно 3 300 ТБ места в год, возьмем плашки по 128 GB, в сервер можно ставить по 8 плашек, итого 3222 сервера, что точно выдержит нашу нагрузку в 300 000 rps, но это не очень оптимально по бюджету, поэтому будем устаревшие сообщения хранить в соседнем сервере на postgres, поскольку пользователи их запрашивают не так часто, если предположить, что это примерно половина всех наших сообщений, то это нам секономин 1500TB хранилища год, и того выйдет 1758 серверов, учитывая репликацию

Итого на 500млн пользователей нам необходимо

    500 000 000 * 40 = 17 ГБ
    
Что даже с учетом резкого роста аудитории спокойно укладывается в один сервер с 128 ГБ оперативной памяти и легко мастабируется

Итоговая конфигурация:
| Параметр| Значение |
| :---: | :---: |
| CPU | 2 ядра |
| Network | 40 Гбит/с |
| RAM | 1024 gb |
| HDD | 256 TB |

Итого: 
    
    1758 серверов


## Источники:
1. [https://vc.ru/social/195967-auditoriya-telegram-prevysila-500-mln-polzovateley-v-mesyac-eto-na-100-mln-bolshe-chem-v-aprele-2020-goda](https://vc.ru/social/195967-auditoriya-telegram-prevysila-500-mln-polzovateley-v-mesyac-eto-na-100-mln-bolshe-chem-v-aprele-2020-goda)
2. https://backlinko.com/telegram-users#telegram-statistics
3. https://telegram.org/blog/15-billion
4. https://www.businessofapps.com/data/telegram-statistics/
5. https://siteefy.com/telegram-statistics/
6. https://ichip.ru/sovety/7-faktov-o-whatsapp-kotorykh-vy-ne-znaete-191822
7. https://www.openlogic.com/blog/centos-vs-ubuntu#best-choice
8. https://habr.com/ru/company/vk/blog/347026/
9. https://www.consul.io
10. https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/
11. https://www.slideshare.net/BadooDev/rps-go
12. https://aws.amazon.com/ru/premiumsupport/knowledge-center/s3-503-within-request-rate-prefix/
13. https://cloudian.com/blog/s3-storage-behind-the-scenes/
14. https://www.kingston.com/ru/ssd/dc1000b-data-center-boot-ssd
15. https://www.tarantool.io/ru/doc/latest/reference/reference_rock/vshard/vshard_index/
16. https://www.tarantool.io/ru/doc/latest/reference/reference_rock/vshard/vshard_architecture/
17. https://www.affde.com/ru/telegram-users.html
18. https://kod.ru/top-of-telegram-countries/
